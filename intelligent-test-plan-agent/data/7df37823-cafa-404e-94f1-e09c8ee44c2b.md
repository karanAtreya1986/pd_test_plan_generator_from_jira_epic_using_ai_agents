# Test Plan (VWO.com) – VWO Login Dashboard  
**Ticket ID:** VWO‑5 **Summary:** Product Requirements Document – VWO Login Dashboard **Priority:** Medium **Status:** To Do  

---  

## 1. Objective  

Provide a comprehensive, traceable testing approach that validates the **VWO Login Dashboard** against the functional, security, performance, accessibility, and usability requirements defined in the PRD. The goal is to ensure:

* Secure, reliable authentication (email/password, 2FA, SSO, Social login)  
* Seamless user experience across devices, browsers, and accessibility modes  
* Compliance with GDPR, OWASP, and WCAG 2.1 AA standards  
* Performance targets (≤ 2 s page load, 99.9 % availability) are met  

---  

## 2. Scope  

| In‑Scope | Out‑Of‑Scope |
|----------|--------------|
| • Login UI (email, password, “Remember Me”, clickable labels, auto‑focus)  <br>• Real‑time field validation (email format, password strength) <br>• Error handling & messaging <br>• Forgot‑Password flow (token generation, email reset) <br>• Multi‑Factor Authentication (optional 2FA) <br>• Enterprise SSO (SAML/OAuth) <br>• Social login (Google, Microsoft) <br>• Theme support – Light/Dark mode <br>• Accessibility features (ARIA, keyboard navigation, high‑contrast) <br>• Session management (timeout, secure cookies) <br>• Rate‑limiting & brute‑force protection <br>• Performance & load‑time verification <br>• Analytics tracking of login success/failure | • Post‑login dashboard functionality (campaign creation, editor, reporting) <br>• Backend admin panels unrelated to authentication <br>• Third‑party services not directly used for login (e.g., marketing automation) |

---  

## 3. Inclusions  

* **Test Documentation** – Test Plan, Test Scenarios, Test Cases, Traceability Matrix, Defect Reports, Test Summary.  
* **Testing Types** – Functional, UI/UX, Accessibility, Security, Performance, Compatibility, Regression.  
* **Test Levels** – Smoke, Sanity, Full regression cycles.  

---  

## 4. Test Environments  

| Environment | URL | OS / Browser Matrix | Device Types |
|-------------|-----|---------------------|--------------|
| **QA** | `qa.vwo.com` | Windows 10 – Chrome 118, Firefox 118, Edge 118  <br> macOS 13 – Safari 16  <br> Android 13 – Chrome <br> iOS 16 – Safari | Desktop, Laptop, Tablet, Smartphone |
| **Pre‑Prod** | `preprod.vwo.com` | Same matrix as QA | Same |
| **UAT** | `uat.vwo.com` | Same matrix as QA | Same |
| **Prod** | `app.vwo.com` | Same matrix as QA | Same |

*Network:* Wired 1 Gbps, Wi‑Fi 100 Mbps, 4G LTE (≥ 20 Mbps) – to verify load‑time under varied bandwidth.  

---  

## 5. Defect Reporting Procedure  

| Step | Action |
|------|--------|
| **1** | Verify defect against **Entry Criteria** (reproducible, logged, attached evidence). |
| **2** | Log defect in **JIRA** using the “VWO‑Login‑Bug” issue type. Required fields: Summary, Description, Steps to Reproduce, Expected vs. Actual, Screenshots/Logs, Environment, Severity, Priority, Tested By. |
| **3** | Assign to **Backend** (Sonal) for API‑related issues, **Frontend** (Devesh) for UI/UX, **DevOps** (Prajeeth) for environment/configuration. |
| **4** | Triage meeting (twice daily) to set **Severity** (Critical, Major, Minor, Trivial) and **Priority** (P1‑P4). |
| **5** | Developer updates status; QA retests after fix and closes defect with “Verified”. |
| **6** | End‑of‑day summary email sent to stakeholders (Product Owner, QA Lead, Dev Lead). |

---  

## 6. Test Strategy  

### 6.1 Test Design Techniques  

| Technique | Application |
|-----------|-------------|
| **Equivalence Class Partitioning (ECP)** | Email & password inputs (valid, invalid formats, empty). |
| **Boundary Value Analysis (BVA)** | Password length (min 8, max 64 characters). |
| **Decision Table Testing** | Combination of “Remember Me”, “2FA enabled”, “SSO selected”. |
| **State Transition** | Login → Session Active → Timeout → Re‑login. |
| **Use‑Case Testing** | New user registration → first login → dashboard. |
| **Error Guessing** | Injection of special characters, script tags, extremely long strings. |
| **Exploratory Testing** | Ad‑hoc checks on theme switching, screen‑reader navigation. |

### 6.2 Test Types  

| Type | Purpose | Tools |
|------|---------|-------|
| **Smoke / Sanity** | Verify build stability before full regression. | Cypress (smoke scripts) |
| **Functional / UI** | Validate all login flows, validation messages, UI elements. | Selenium WebDriver, Cypress |
| **Accessibility** | Verify WCAG 2.1 AA compliance (ARIA, keyboard, contrast). | axe‑core, Wave |
| **Security** | OWASP Authentication testing, brute‑force, session hijacking. | OWASP ZAP, Burp Suite |
| **Performance / Load** | Page‑load ≤ 2 s, concurrent login ≥ 2000 users. | JMeter, Locust |
| **Regression** | Re‑run all functional tests after each build. | Cypress, TestRail for tracking |
| **Cross‑Browser / Device** | Ensure consistent UI/UX across matrix. | BrowserStack, Sauce Labs |

### 6.3 Test Execution Flow  

1. **Smoke Test** on each new build.  
2. **Sanity** of critical login paths (email/password, 2FA, SSO).  
3. **Full Functional Suite** (positive, negative, edge cases).  
4. **Accessibility** checks on Light/Dark themes.  
5. **Security** scan (once per sprint).  
6. **Performance** test on pre‑prod (once per sprint).  
7. **Regression** cycle (after defect fixes).  

---  

## 7. Test Schedule  

| Sprint | Activity | Start | End |
|--------|----------|-------|-----|
| **Sprint 1** | Test Plan finalisation, environment provisioning | 2026‑02‑15 | 2026‑02‑19 |
| | Test Scenario & Test Case creation | 2026‑02‑20 | 2026‑02‑24 |
| | Smoke / Sanity execution (QA) | 2026‑02‑25 | 2026‑02‑26 |
| | Functional testing – Core Auth (email/password) | 2026‑02‑27 | 2026‑03‑04 |
| **Sprint 2** | Functional testing – 2FA, SSO, Social login | 2026‑03‑05 | 2026‑03‑10 |
| | Accessibility & UI testing (Light/Dark) | 2026‑03‑11 | 2026‑03‑13 |
| | Security testing (OWASP) | 2026‑03‑14 | 2026‑03‑16 |
| | Performance / Load testing | 2026‑03‑17 | 2026‑03‑19 |
| | Regression cycle & defect retest | 2026‑03‑20 | 2026‑03‑22 |
| | Test Summary Report & Sign‑off | 2026‑03‑23 | 2026‑03‑24 |

*All dates are provisional and may shift based on build availability.*

---  

## 8. Test Deliverables  

| Deliverable | Owner | Due |
|-------------|-------|-----|
| Test Plan (this document) | QA Lead | 2026‑02‑15 |
| Test Scenarios & Traceability Matrix | Senior QA | 2026‑02‑24 |
| Detailed Test Cases (incl. data) | QA Team | 2026‑02‑24 |
| Smoke / Sanity Test Scripts | Automation Engineer | 2026‑02‑26 |
| Functional Test Execution Report | QA Analyst | 2026‑03‑04 |
| Accessibility Report (axe) | QA Analyst | 2026‑03‑13 |
| Security Scan Report (ZAP) | Security QA | 2026‑03‑16 |
| Performance Test Report (JMeter) | Performance Engineer | 2026‑03‑19 |
| Defect Log (JIRA) | All Testers | Ongoing |
| Test Summary & Sign‑off Document | QA Lead | 2026‑03‑24 |

---  

## 9. Entry & Exit Criteria  

### 9.1 Entry Criteria (Test Planning Phase)  

* PRD & functional specs approved.  
* Test environment URLs functional & accessible.  
* Test data (valid/invalid emails, passwords, 2FA tokens) prepared.  
* Automation framework set up (Cypress, JMeter).  

### 9.2 Exit Criteria (Test Completion)  

* All **Test Cases** executed with ≥ 95 % pass rate.  
* Critical defects (Severity = Critical) resolved and verified.  
* No high‑severity accessibility violations (WCAG 2.1 AA).  
* Performance targets met (≤ 2 s load, ≤ 5 % error rate at 2000 concurrent users).  
* Test Summary Report signed off by Product Owner & QA Lead.  

---  

## 10. Test Execution  

### 10.1 Entry Criteria  

* Test Cases signed‑off by client.  
* Build deployed to **QA** environment and smoke‑tested passed.  

### 10.2 Exit Criteria  

* Execution logs uploaded to TestRail.  
* Defect report generated and shared.  
* All test data archived for audit.  

---  

## 11. Test Closure  

### 11.1 Entry Criteria  

* All test cycles completed.  
* All defects either closed or deferred with justification.  

### 11.2 Exit Criteria  

* Test Closure Report compiled (metrics, open risks, lessons learned).  
* All deliverables archived in the project repository.  
* Formal sign‑off obtained from Stakeholders (Product Owner, Security Lead, Dev Lead).  

---  

## 12. Tools  

| Category | Tool | Purpose |
|----------|------|---------|
| **Test Management** | TestRail / Zephyr | Test case authoring, execution tracking |
| **Defect Tracking** | JIRA | Defect lifecycle |
| **Automation** | Cypress, Selenium WebDriver | Functional UI regression |
| **Performance** | Apache JMeter, Locust | Load & stress testing |
| **Security** | OWASP ZAP, Burp Suite | Vulnerability scanning |
| **Accessibility** | axe‑core (Chrome extension), Wave | WCAG compliance |
| **Cross‑Browser** | BrowserStack, Sauce Labs | Device & browser matrix |
| **Collaboration** | Confluence, Slack | Documentation & communication |
| **Data Generation** | Faker, CSV files | Test data for emails, passwords, tokens |

---  

## 13. Risks & Mitigations  

| Risk | Impact | Likelihood | Mitigation |
|------|--------|------------|------------|
| **Resource unavailability** (e.g., key QA leaves) | Delay in test execution | Medium | Cross‑train team members; maintain a bench of backup QA. |
| **Build instability / URL not reachable** | Blocked testing | High | Early integration of CI pipeline; fallback to previous stable build. |
| **Third‑party SSO provider downtime** | Incomplete SSO coverage | Low | Mock SSO responses; schedule SSO testing in a window with provider SLA. |
| **Performance test overload on shared environment** | False negatives | Medium | Use dedicated load‑test environment; schedule off‑peak. |
| **Regulatory compliance changes (GDPR/CCPA)** | Re‑work of data handling tests | Low | Keep compliance checklist up‑to‑date; involve Legal early. |
| **Accessibility tool false‑positives** | Wasted effort | Low | Validate findings manually with screen readers (NVDA, VoiceOver). |

---  

## 14. Approvals  

| Role | Name | Signature | Date |
|------|------|-----------|------|
| **Product Owner** |  |  |  |
| **QA Lead** |  |  |  |
| **Dev Lead** |  |  |  |
| **Security Lead** |  |  |  |
| **Compliance Lead** |  |  |  |

*Testing will proceed only after all signatures are obtained.*  

---  

## 15. Test Cases (Sample)  

| TC ID | Requirement | Test Scenario | Pre‑conditions | Steps | Expected Result | Test Data |
|-------|-------------|---------------|----------------|-------|----------------|-----------|
| **TC‑001** | **FR‑1** – Email & Password login | Valid email & password login (positive) | User exists, email verified, password active | 1. Navigate to `app.vwo.com` <br>2. Verify auto‑focus on Email field <br>3. Enter valid email <br>4. Enter valid password <br>5. Click **Login** | Dashboard loads within 2 s, session cookie set, “Welcome <User>” displayed | Email: `john.doe@example.com` <br>Pwd: `Vw0$ecure!23` |
| **TC‑002** | **FR‑1** | Invalid email format | – | 1. Enter `john.doe@` <br>2. Tab out of field | Inline validation message “Enter a valid email address” appears; **Login** button disabled | Email: `john.doe@` |
| **TC‑003** | **FR‑1** | Empty password field | Valid email entered | 1. Leave password blank <br>2. Click **Login** | Error toast “Password is required” displayed; login not attempted | Email: `john.doe@example.com` |
| **TC‑004** | **FR‑2** – Password Strength Indicator | Verify strength bar updates on each character | – | 1. Focus Password field <br>2. Type `a` → observe “Very Weak” <br>3. Type `aB3$` → observe “Weak” <br>4. Type `aB3$xyz!` → observe “Strong” | Visual indicator reflects strength as per policy (min 8 chars, mix) | Incremental passwords |
| **TC‑005** | **FR‑3** – Remember Me | Session persists after browser close when checked | – | 1. Check **Remember Me** <br>2. Login with valid credentials <br>3. Close browser <br>4. Re‑open browser and navigate to `app.vwo.com` | User is still authenticated, redirected to dashboard without login prompt | Same as TC‑001 |
| **TC‑006** | **FR‑3** – Remember Me unchecked | Session expires on browser close | – | 1. Ensure **Remember Me** unchecked <br>2. Login <br>3. Close browser <br>4. Re‑open and navigate | Login page displayed; user must re‑authenticate | – |
| **TC‑007** | **FR‑4** – Forgot Password (positive) | Successful password reset email sent | User exists, email verified | 1. Click **Forgot password?** <br>2. Enter registered email <br>3. Submit <br>4. Check inbox for reset link | Email received within 60 s, link contains secure token, clicking opens Reset Password page | Email: `john.doe@example.com` |
| **TC‑008** | **FR‑4** – Forgot Password (invalid email) | Error for unregistered email | – | 1. Click **Forgot password?** <br>2. Enter `nonexistent@domain.com` <br>3. Submit | Inline error “Email not found” displayed; no email sent | Email: `nonexistent@domain.com` |
| **TC‑009** | **FR‑5** – 2FA (OTP) | Successful OTP verification | User has 2FA enabled, receives OTP via email/SMS | 1. Login with valid credentials <br>2. Prompt for OTP displayed <br>3. Enter correct OTP <br>4. Click **Verify** | Dashboard loads; session marked as “2FA‑verified” | OTP: `123456` (mocked) |
| **TC‑010** | **FR‑5** – 2FA (invalid OTP) | OTP rejection | Same as TC‑009 | 1. Enter wrong OTP `000000` <br>2. Submit | Error “Invalid code, try again”; login blocked after 3 attempts (rate‑limit) | – |
| **TC‑011** | **FR‑6** – SSO (SAML) | Successful SSO login using corporate IdP | IdP test account `employee@corp.com` | 1. Click **Login with SSO** <br>2. Select **SAML** <br>3. Authenticate on IdP portal <br>4. Redirect back to VWO | Dashboard appears; SSO session cookie set | IdP credentials |
| **TC‑012** | **FR‑7** – Social Login (Google) | Google OAuth login | Google test account | 1. Click **Login with Google** <br>2. Select Google account <br>3. Grant consent <br>4. Redirect back | Dashboard loads; Google token stored securely | Google test user |
| **TC‑013** | **