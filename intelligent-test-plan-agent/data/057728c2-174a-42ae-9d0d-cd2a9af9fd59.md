# Universal Test Plan  
**Project Name:** VWO Application – Login & Core UI Epic  
**Version:** 1.0  
**Prepared By:** *[QA Lead – Name]*  
**Date:** 2026‑02‑15  
**Approved By:** *[Stakeholder – Name]*  

---  

## 1. Introduction  

### Purpose  
The purpose of this test plan is to define the testing approach, activities, resources, and schedule required to verify the **VWO login functionality** and the immediate post‑login user interface (Dashboard, Campaign Creation, Editor, and “Run on a Website” flow). The plan ensures that the product meets the functional, usability, security, and performance expectations defined for the VWO platform.

### Scope  

| **In‑Scope** | **Out‑Of‑Scope** |
|--------------|------------------|
| • Login page (email/username, password, “Remember Me”, “Forgot Password”)  <br>• Authentication flow (session creation, token handling) <br>• Dashboard landing after successful login <br>• Navigation to **Create New Campaign**, **Editor**, and **Run on a Website** screens <br>• UI/UX validation (layout, responsiveness, accessibility) <br>• Cross‑browser & device compatibility (Chrome, Firefox, Edge, Safari, Android Chrome, iOS Safari) <br>• Basic performance of login (≤2 s response) | • Deep performance/load testing of the entire VWO platform <br>• Third‑party integrations (e.g., analytics providers) <br>• Backend data‑migration or reporting modules not directly invoked from the login flow <br>• Internationalisation/localisation beyond English |

---  

## 2. References  

| # | Document | Link / Location |
|---|----------|-----------------|
| R1 | Requirement Specification – VWO Login Epic | Confluence > VWO > Requirements > SCRUM‑5 |
| R2 | UI/UX Design Mock‑ups (Login, Dashboard, Campaign Editor) | Figma project “VWO‑Login‑UI” |
| R3 | User Stories (Login, Dashboard, Campaign Creation) | JIRA > Epic SCRUM‑5 |
| R4 | Technical Architecture – React 18.2, jQuery 2.1.1, PostgreSQL | Confluence > Architecture > VWO Backend |
| R5 | Security Guidelines – OWASP Top 10 for authentication | Internal Security Wiki |

---  

## 3. Test Objectives  

1. **Functional Verification** – Confirm that the login process authenticates valid users and rejects invalid credentials according to requirements.  
2. **UI/UX Validation** – Ensure all UI elements render correctly across supported browsers/devices and meet accessibility standards (WCAG 2.1 AA).  
3. **Security Checks** – Verify that passwords are never logged, that account lockout works, and that session tokens are securely stored.  
4. **Performance Baseline** – Measure login response time under nominal load (≤2 s) and confirm no regression.  
5. **Reliability** – Validate that the system gracefully handles network interruptions, session expiry, and “Remember Me” functionality.  
6. **Defect Identification** – Detect and log any deviations from the documented requirements or user expectations.

---  

## 4. Test Items  

| ID | Component / Feature | Description |
|----|---------------------|-------------|
| TI‑01 | **Login Page** | Email/username field, password field, “Remember Me” checkbox, “Forgot Password?” link, Sign‑In button |
| TI‑02 | **Authentication Service** | REST API `/api/auth/login`, token generation, session cookie handling |
| TI‑03 | **Dashboard** | Post‑login landing page – navigation menu, quick‑stats widgets |
| TI‑04 | **Create New Campaign** | Button/link from Dashboard, campaign wizard start |
| TI‑05 | **Campaign Editor** | UI for defining variations, targeting rules |
| TI‑06 | **Run on a Website** | Integration snippet generation, copy‑to‑clipboard, validation of snippet placement |

---  

## 5. Test Strategy  

| **Test Level** | **Scope** |
|----------------|-----------|
| **Unit** | Covered by developers (React components, API endpoints). Not part of this plan. |
| **Integration** | Verify interaction between UI and authentication API (mocked & real). |
| **System** | End‑to‑end validation of login → dashboard → campaign flow on supported environments. |
| **UAT** | Business stakeholder acceptance of login experience and navigation. |

| **Test Types** | **Approach** |
|----------------|--------------|
| Functional | Manual execution of test cases; automated regression for smoke suite (Cypress). |
| Regression | Run full suite on each new build after smoke pass. |
| Performance | Simple load test (JMeter) for 50 concurrent login attempts. |
| Security | Manual checks for password handling, OWASP‑based token tests. |
| Accessibility | Axe‑core automated scans + manual keyboard navigation. |
| Compatibility | Matrix testing across browsers & devices (see Section 7). |

**Design Techniques** – Equivalence Partitioning, Boundary Value Analysis, Decision Table (role‑based access), State Transition (login → lockout → unlock), Use‑Case, Error Guessing, Exploratory.

---  

## 6. Test Environment  

| **Component** | **Details** |
|---------------|-------------|
| **Hardware** | Standard test workstation: Intel i7, 16 GB RAM, SSD. Mobile devices: iPhone 14 (iOS 17), Samsung Galaxy S23 (Android 13). |
| **Operating Systems** | Windows 10, macOS 14 (Ventura), iOS 17, Android 13. |
| **Browsers / Devices** | Chrome 120, Firefox 121, Edge 120, Safari 17 (macOS), Safari iOS 17, Chrome Android 120. |
| **Database** | PostgreSQL 15 – pre‑populated with test user accounts (see Test Data). |
| **Web Servers** | Nginx 1.24 (reverse proxy) + Apache 2.4 (static assets) – as per architecture. |
| **Network** | Wired LAN (1 Gbps) for desktop; Wi‑Fi (≥30 Mbps) for mobile. |
| **Security** | HTTPS enforced; authentication tokens stored in HttpOnly, Secure cookies. |
| **Environment URLs** | QA – `https://qa.vwo.com`  <br>Pre‑Prod – `https://preprod.vwo.com`  <br>UAT – `https://uat.vwo.com`  <br>Prod – `https://vwo.com` |

---  

## 7. Test Tools  

| **Tool** | **Purpose** |
|----------|-------------|
| **Test Management** | JIRA (Test case creation, execution tracking) |
| **Automation** | Cypress (UI end‑to‑end), Postman (API sanity) |
| **Performance** | Apache JMeter (login load) |
| **Bug Tracking** | JIRA (Defect workflow) |
| **Accessibility** | axe‑core (Chrome extension) |
| **Screen Capture** | Snipping Tool / Lightshot |
| **Documentation** | Microsoft Word / Excel (Traceability matrix) |

---  

## 8. Entry and Exit Criteria  

### Entry Criteria  

| # | Condition |
|---|-----------|
| EC‑01 | All functional requirements for login (R1‑R5) are approved and baselined. |
| EC‑02 | Stable build deployed to the target environment (QA) with version tag `v1.0‑login`. |
| EC‑03 | Test data (user accounts) are loaded into the test database. |
| EC‑04 | Test environment (browsers, devices) is verified as available. |
| EC‑05 | Test cases are reviewed and signed‑off by the Test Lead. |

### Exit Criteria  

| # | Condition |
|---|-----------|
| EX‑01 | ≥ 95 % of test cases executed with Pass status. |
| EX‑02 | No **Critical** or **High** severity defects remain open (unresolved) after retest. |
| EX‑03 | All **Medium** defects are either fixed or accepted with a documented mitigation. |
| EX‑04 | Performance test shows average login response ≤ 2 seconds under 50 concurrent users. |
| EX‑05 | Accessibility scan reports ≤ 3 AA‑level violations (must be addressed). |
| EX‑06 | Test Summary Report signed‑off by Test Lead and Product Owner. |

---  

## 9. Test Deliverables  

| # | Deliverable | Owner | Due |
|---|-------------|-------|-----|
| D1 | Test Plan (this document) | Test Lead | 2026‑02‑15 |
| D2 | Test Cases (JIRA) | QA Engineers | 2026‑02‑20 |
| D3 | Automated Smoke Scripts (Cypress) | Automation Engineer | 2026‑02‑22 |
| D4 | Execution Log & Test Run Report | QA Engineers | End of each sprint |
| D5 | Defect Report (JIRA) | QA Engineers | Ongoing |
| D6 | Test Summary Report | Test Lead | 2026‑03‑10 |
| D7 | Test Data Sheet (user credentials) | Test Data Owner | 2026‑02‑18 |

---  

## 10. Test Schedule  

| Phase | Activities | Start | End |
|-------|------------|-------|-----|
| **Planning** | Create test plan, test case design, test data preparation | 2026‑02‑15 | 2026‑02‑20 |
| **Design** | Review & sign‑off test cases, develop automation scripts | 2026‑02‑21 | 2026‑02‑28 |
| **Execution – Sprint 1** | Smoke testing, functional testing of login & dashboard | 2026‑03‑01 | 2026‑03‑05 |
| **Execution – Sprint 2** | Full regression, performance, accessibility, mobile testing | 2026‑03‑06 | 2026‑03‑12 |
| **Closure** | Defect retesting, metrics collection, final reporting | 2026‑03‑13 | 2026‑03‑15 |
| **Buffer** | Contingency for build issues | 2026‑03‑16 | 2026‑03‑18 |

---  

## 11. Roles and Responsibilities  

| Role | Person(s) | Responsibilities |
|------|-----------|------------------|
| **Test Manager / Lead** | *[Name]* | Overall test planning, risk management, sign‑off of deliverables. |
| **QA Engineer** | *[Names]* | Test case creation, execution, defect logging, regression testing. |
| **Automation Engineer** | *[Name]* | Develop/maintain Cypress smoke suite, integrate with CI. |
| **Performance Engineer** | *[Name]* | Design & run JMeter scripts, analyze results. |
| **Developer (Frontend)** | Devesh | Fix UI defects, support exploratory testing. |
| **Developer (Backend)** | Sonal | Resolve authentication API issues, assist with security validation. |
| **DevOps** | Prajeeth | Provision environments, ensure URLs are reachable, manage test data loads. |
| **Product Owner** | *[Name]* | Clarify acceptance criteria, approve test results. |
| **Stakeholder / Business Analyst** | *[Name]* | Participate in UAT, validate business flows. |

---  

## 12. Resource Planning  

| Resource | Quantity | Comments |
|----------|----------|----------|
| QA Engineers | 3 | One dedicated to mobile testing. |
| Automation Engineer | 1 | Part‑time (30 % effort). |
| Performance Engineer | 1 | Part‑time. |
| Test Devices | 2 laptops (Win10, macOS), 2 smartphones (iPhone, Android) | Pre‑configured with required browsers. |
| Licenses | JIRA, Cypress, JMeter, axe‑core | Already available in the organization. |
| Time | ~ 4 weeks total (including buffer) | Aligned with two‑sprint schedule. |

---  

## 13. Risk and Mitigation Plan  

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| **R1 – Resource unavailability** (e.g., sick leave) | Delay in execution | Medium | Cross‑train two backup QA engineers; maintain up‑to‑date test artefacts. |
| **R2 – Build URL not reachable** | Cannot start testing | Low | Pre‑Prod & QA environments have fallback URLs; if unavailable, shift focus to API‑level testing with mock server. |
| **R3 – Limited time for testing** | Reduced coverage | Medium | Prioritize critical test cases (login, security); use risk‑based testing; increase parallel execution on multiple devices. |
| **R4 – Third‑party authentication service latency** | False negatives in performance | Low | Mock the auth service for functional tests; run separate performance test against real service when available. |
| **R5 – Browser version incompatibility** | UI defects missed | Low | Maintain a browser version matrix; use BrowserStack for additional coverage. |

---  

## 14. Defect Management Process  

1. **Log Defect** – Create a JIRA issue using the “VWO‑Bug” template. Include steps to reproduce, screenshots, environment, and severity.  
2. **Assign Severity / Priority** – QA Lead assigns based on impact (Critical, High, Medium, Low).  
3. **Triage** – Development team reviews within 4 hours of logging.  
4. **Fix** – Developer resolves and updates the defect status to *In Progress*.  
5. **Retest** – QA re‑executes the original test case; if passed, mark *Resolved*.  
6. **Close** – After verification by Test Lead and Product Owner, status set to *Closed*.  
7. **Metrics** – Daily defect aging report sent to stakeholders; trend analysis included in weekly status meeting.

---  

## 15. Metrics and Reporting  

| Metric | Definition | Target |
|--------|------------|--------|
| **Test Execution %** | (Executed TC / Total TC) × 100 | ≥ 95 % |
| **Pass Rate** | (Passed TC / Executed TC) × 100 | ≥ 93 % |
| **Defect Density** | Defects / 1 K LOC (approx) | ≤ 0.5 |
| **Critical Defect Leakage** | Critical defects found post‑release | 0 |
| **Average Login Response Time** | Mean time from click → dashboard load | ≤ 2 s |
| **Accessibility Violations (AA)** | Number of failures from axe scan | ≤ 3 |

*Reporting Cadence*:  
- **Daily** – Smoke test status, open critical defects (via JIRA dashboard).  
- **Sprint End** – Execution summary, defect trend, risk register.  
- **Final** – Test Summary Report with all metrics, sign‑off.

---  

## 16. Communication Plan  

| Channel | Frequency | Audience | Owner |
|---------|-----------|----------|-------|
| Daily Stand‑up | 15 min each morning | QA Team, Dev, DevOps | Scrum Master |
| Defect Review Meeting | Twice per sprint (or ad‑hoc for critical bugs) | QA, Developers, PO | Test Lead |
| Weekly Status Email | Every Friday | Project Stakeholders | Test Lead |
| Sprint Demo / UAT Review | End of each sprint | Product Owner, Business Users | QA Lead |
| Test Plan Approval | One‑time (before execution) | PO, QA Manager | Test Lead |

---  

## 17. Assumptions and Constraints  

- **Assumptions**  
  - Requirements for login are frozen for the duration of the two‑sprint testing window.  
  - Test data (user accounts, roles) will be supplied by the DevOps team before **2026‑02‑18**.  
  - All browsers listed will be available on the test machines (or via BrowserStack).  

- **Constraints**  
  - Fixed budget for third‑party device cloud (limited to 5 concurrent sessions).  
  - Only one stable build per sprint is expected; hot‑fixes will be accommodated as separate mini‑cycles.  

---  

## 18. Approval  

| Name | Role | Signature | Date |
|------|------|-----------|------|
| *[Test Lead]* | Test Lead | ___________________ | 2026‑02‑15 |
| *[Product Owner]* | Product Owner | ___________________ | 2026‑02‑15 |
| *[Development Manager]* | Development Manager | ___________________ | 2026‑02‑15 |

---  

## 19. Test Cases (Sample)  

> **Note:** Full test case repository resides in JIRA (project VWO‑QC). Below is a representative subset mapped to inferred acceptance criteria.

| TC ID | Title | Requirement(s) | Preconditions | Steps | Expected Result | Priority |
|-------|-------|----------------|---------------|-------|-----------------|----------|
| TC‑001 | Valid Login – Email | REQ‑001 (Valid credentials) | User **john.doe@example.com** with password **Password123** exists & is active. | 1. Navigate to `https://qa.vwo.com/login` <br>2. Enter email and password <br>3. Click **Sign In** | User is redirected to Dashboard within 2 s; session cookie set (HttpOnly, Secure); Dashboard widgets load. | High |
| TC‑002 | Valid Login – Username | REQ‑001 | User **jdoe** (username) with password **Password123** exists. | Same as TC‑001, but use username field. | Same as TC‑001. | High |
| TC‑003 | Invalid Login – Wrong Password | REQ‑002 (Reject invalid credentials) | Same user as TC‑001. | 1. Enter correct email, wrong password **WrongPass!** <br>2. Click **Sign In** | Error message displayed: “Invalid email or password.” No session created. | High |
| TC‑004 | Invalid Login – Non‑existent Account | REQ